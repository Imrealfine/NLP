{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imyaVBbcIBVq"
      },
      "source": [
        "**Másteres Universitarios en Ciencia de Datos, Ingeniería Informática, e Innovación en Inteligencia Computacional y Sistemas Interactivos, UAM**\n",
        "## **Procesamiento del Lenguaje Natural**\n",
        "# **Práctica de laboratorio 4: Ajuste fino de BERT para análisis de sentimientos**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nwr3C40gc87"
      },
      "source": [
        "En esta práctica, vamos a ajustar (hacer *fine-tuning* de) BERT para la tarea de análisis de sentimiento de la práctica 3.\n",
        "\n",
        "Al hacerlo, estaremos posibilitando la transferencia de aprendizaje (*transfer learning*) de un modelo de lenguaje preentrenado, lo cual ha quedado demostrado ser una estrategia exitosa para lograr rendimientos de última generación en tareas de procesamiento del lenguaje natural.\n",
        "\n",
        "Aprenderemos lo siguiente:\n",
        "\n",
        "*   Ajustar modelos transformers de la biblioteca [Hugging Face](https://github.com/huggingface/transformers).\n",
        "*   Preprocesar datos para la arquitectura de transformers (tokenización de subpalabras con WordPiece).\n",
        "*   Implementar un clasificador basado en BERT.\n",
        "\n",
        "Como aprendimos en la clase de teoría, utilizaremos el conocimiento codificado en el Transformer para mejorar el aprendizaje de nuestra tarea objetivo. Así, nuestro clasificador de sentimientos tendrá dos componentes principales:\n",
        "\n",
        "*   El codificador de texto (encoder) BERT, que no sabe nada sobre sentimientos, pero sí sobre el idioma inglés.\n",
        "*   Un componente dedicado a la clasificación de sentimientos, que será una capa feed-forward.\n",
        "\n",
        "BERT generará los embeddings para las frases entrada y los pasará a la capa de clasificación. Cuando ajustemos el clasificador, también modificaremos los parámetros de BERT para que aprenda aspectos específicos de la tarea.\n",
        "\n",
        "Ventajas de esta arquitectura y del aprendizaje por transferencia:\n",
        "\n",
        "*   Se puede recopilar una cantidad ilimitada de texto no etiquetado desde la web con muy poco esfuerzo para entrenar un modelo de lenguaje a gran escala.\n",
        "*   Transformer es una arquitectura feed-forward que permite un entrenamiento altamente paralelizado y eficiente en conjuntos de datos masivos, con el objetivo de predecir palabras basadas en su contexto ([consulta el tutorial sobre estrategias de aprendizaje para clasificación de secuencias](https://colab.research.google.com/drive/1yWaLpCWImXZE2fPV0ZYDdWWI8f52__9A#scrollTo=MGqVkG2-7qfu)).\n",
        "*   Aunque el preentrenamiento de un modelo de lenguaje puede ser costoso, el *fine-tuning* generalmente puede realizarse en una sola GPU, ya que típicamente requiere pocas épocas de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWHDCCqPIBVr"
      },
      "source": [
        "## 1. Carga de los datos\n",
        "Usaremos los mismos datos de análisis de sentimiento que en la práctica anterior. Por lo tanto, primero necesitamos montar nuestra cuenta de Google Drive, y después acceder al Stanford Sentiment Treebank (SST)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihLkOMT9OJDt"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZhrDTCkIBVt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "## Fijamos semillas de generación de números aleatorios para replicabilidad de resultados\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "# Utilizamos el Stanford Sentiment Treebank (SST) como corpus de prueba para análisis de sentimientos de textos\n",
        "# Cargamos y procesamos los datos para tratarlos como un problema de clasificación binaria de sentimientos\n",
        "# Convertimos la escala de sentimientos de [1,5] a {0,1}\n",
        "def load_sst_data(path,\n",
        "                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        for i, line in enumerate(f):\n",
        "            example = {}\n",
        "            example['label'] = easy_label_map[int(line[1])]\n",
        "            if example['label'] is None:\n",
        "                continue\n",
        "\n",
        "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
        "            example['text'] = text[1:]\n",
        "            data.append(example)\n",
        "    data = pd.DataFrame(data)\n",
        "    return data\n",
        "\n",
        "def pretty_print(example):\n",
        "    print('Label: {}\\nText: {}'.format(example['label'], example['text']))\n",
        "\n",
        "sst_home = '/media/dong/三星disk1/Users/Xudon/Documents/GitHub/NLP/Lab_3/nlp2425-lab3_data/data/'\n",
        "training_set = load_sst_data(sst_home+'/sst_training.txt')\n",
        "validation_set = load_sst_data(sst_home+'/sst_validation.txt')\n",
        "test_set = load_sst_data(sst_home+'/sst_test.txt')\n",
        "\n",
        "# Desordenamos los datos\n",
        "training_set = shuffle(training_set)\n",
        "validation_set = shuffle(validation_set)\n",
        "test_set = shuffle(test_set)\n",
        "\n",
        "# Obtenemos los vectores de texto y etiquetas (clases de opinión: positiva y negativa)\n",
        "train_texts = training_set.text\n",
        "train_labels = training_set.label\n",
        "\n",
        "validation_texts = validation_set.text\n",
        "validation_labels = validation_set.label\n",
        "\n",
        "test_texts = test_set.text\n",
        "test_labels = test_set.label\n",
        "\n",
        "print('Tamaño del conjunto de entrenamiento: {}'.format(len(training_set)))\n",
        "print('Tamaño del conjunto de validación: {}'.format(len(validation_set)))\n",
        "print('Tamaño del conjunto de test: {}'.format(len(test_set)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZqxg504JNIv"
      },
      "source": [
        "## 2. Instalación y configuración de la librería Transformers\n",
        "Para usar la librería Transformers, deberemos registrarnos en Hugging Face y obtener una API key, que llamaremos HF_TOKEN. Esta clave nos permitirá acceder a modelos preentrenados y otros recursos de Hugging Face.\n",
        "\n",
        "Para guardar y activar HF_TOKEN en Google Colab:\n",
        "\n",
        "\n",
        "1.   Accede a Hugging Face Tokens y genera un nuevo token de acceso con permisos de lectura.\n",
        "2.   Para evitar compartir tu clave de acceso en el código Python, puedes añadir HF_TOKEN como un \"secreto\" en Google Colab. Para hacerlo, sigue estos pasos:\n",
        "    *   En Google Colab, haz clic en el icono de su panel de control en la parte izquierda.\n",
        "    *   Luego, selecciona la pestaña \"Secretos\".\n",
        "    *   Haz clic en \"Nuevo secreto\" y en el campo \"Nombre\" escribe HF_TOKEN. En el campo \"Valor\", pega tu clave de Hugging Face que obtuviste en el paso 1.\n",
        "\n",
        "Después de guardar el secreto, en el código podrías acceder al valor de HF_TOKEN como sigue:\n",
        "```\n",
        "import os\n",
        "HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "from huggingface_hub import login\n",
        "login(HF_TOKEN)\n",
        "```\n",
        "\n",
        "De esta manera, usarías la clave de manera segura y acceder a modelos y datasets de Hugging Face sin exponerla directamente en el código. Si has añadido HF_TOKEN en \"Secretos\" de Google Colab, no es necesario añadir las instrucciones anteriores en el código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I1LnTA4II71"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4ZJAgAVelDa"
      },
      "source": [
        "Una vez que la librería Transformers está instalada, podemos utilizarla directamente creando objetos de las siguientes clases:\n",
        "\n",
        "- La clase **tokenizer**: se encarga de convertir una secuencia de entrada en tensores de enteros, que son índices en el vocabulario de un modelo. La tokenización varía según el modelo, ya que cada modelo tiene su propio tokenizer.\n",
        "\n",
        "- La clase **model**: contiene la lógica del modelo de red neuronal en sí. Al utilizar un modelo de TensorFlow, hereda de tf.keras.layers.Layer, lo que significa que se puede usar de manera muy sencilla con la API fit de Keras o para hacer cosas más complicadas.\n",
        "\n",
        "También existe una clase de configuración (**configuration**), que es necesaria a menos que no se estén utilizando los valores predeterminados. Con esta clase indicamos todo lo relacionado con los hiperparámetros del modelo, como el número de capas, dropout, etc. A continuación, se muestra un ejemplo de un archivo de configuración de BERT, para los pesos preentrenados bert-base-cased.\n",
        "\n",
        "```\n",
        "{\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.1,\n",
        "  \"hidden_size\": 768,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 3072,\n",
        "  \"max_position_embeddings\": 512,\n",
        "  \"num_attention_heads\": 12,\n",
        "  \"num_hidden_layers\": 12,\n",
        "  \"type_vocab_size\": 2,\n",
        "  \"vocab_size\": 28996\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfIxoZqYsre-"
      },
      "source": [
        "Específicamente, importamos las clases `TFBertForSequenceClassification` y `BertTokenizer` desde la librería transformers. Utilizamos el método `from_pretrained` para cargar el modelo BERT preentrenado, en este caso el modelo \"bert-base-uncased\", que es una versión de BERT entrenada sin distinción de mayúsculas y minúsculas.\n",
        "\n",
        "\n",
        "*   La variable `model` contiene el modelo BERT adaptado para tareas de clasificación de secuencias, que es el caso que nos interesa aquí pues buscamos clasificar de forma binaria frases atendiendo a sus polaridades de sentimiento, positiva o negativa.\n",
        "*   Por su parte, el `tokenizer` es el que fue empleado para construir BERT, y el que utilizaremos para convertir el texto de entrada en tensores de índices que el modelo puede procesar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7FLj9htJFJW"
      },
      "outputs": [],
      "source": [
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ORrAiuzt_iX"
      },
      "source": [
        "⬛\n",
        "####EJERCICIO 1\n",
        "Atendiendo a lo aprendido en la clase de teoría y a información disponible en la web, ¿cuál es la arquitectura interna de `TFBertForSequenceClassification` con respecto a la de `TFBertModel`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S3Nq-vlu565"
      },
      "source": [
        "[RESPONDER AQUÍ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U_bMFqMcINb"
      },
      "source": [
        "## 3. Preprocesado de los textos\n",
        "A continuación, definimos dos funciones auxiliares para 1) extraer características del tokenizer (`convert_examples_to_features`) y 2) convertir las características en un objeto de la clase `tf.data.Dataset `(`convert_features_to_tf_dataset`).\n",
        "\n",
        "`tf.data.Dataset` ayuda a gestionar e iterar de manera eficiente los datos de entrada y salida del modelo. Para más información, puedes consultar la API en la página web de TensorFlow: https://www.tensorflow.org/api_docs/python/tf/data/Dataset.\n",
        "\n",
        "Con esas funciones preprocesamos los conjuntos de entrenamiento y validación. Usamos `tf.data.Dataset` para establecer el tamaño de lote en 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAolNiqWJmM3"
      },
      "outputs": [],
      "source": [
        "from transformers import InputFeatures\n",
        "\n",
        "def convert_examples_to_features(texts, labels):\n",
        "  labels = list(labels)\n",
        "  batch_encoding = tokenizer.batch_encode_plus(texts, max_length=128, pad_to_max_length=True)\n",
        "\n",
        "  features = []\n",
        "  for i in range(len(texts)):\n",
        "      inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "\n",
        "      feature = InputFeatures(**inputs, label=labels[i])\n",
        "      features.append(feature)\n",
        "\n",
        "  for i, example in enumerate(texts[:5]):\n",
        "      print(\"*** Ejemplo ***\")\n",
        "      print(\"texto: %s\" % (example))\n",
        "      print(\"características: %s\" % features[i])\n",
        "\n",
        "  return features\n",
        "\n",
        "def convert_features_to_tf_dataset(features):\n",
        "  def gen():\n",
        "      for ex in features:\n",
        "          yield (\n",
        "              {\n",
        "                  \"input_ids\": ex.input_ids,\n",
        "                  \"attention_mask\": ex.attention_mask,\n",
        "                  \"token_type_ids\": ex.token_type_ids,\n",
        "              },\n",
        "              ex.label,\n",
        "          )\n",
        "  dataset = tf.data.Dataset.from_generator(gen,\n",
        "                                           ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "                                           (\n",
        "                                               {\n",
        "                                                \"input_ids\": tf.TensorShape([None]),\n",
        "                                                \"attention_mask\": tf.TensorShape([None]),\n",
        "                                                \"token_type_ids\": tf.TensorShape([None])\n",
        "                                                },\n",
        "                                            tf.TensorShape([]),\n",
        "                                            ))\n",
        "  return dataset\n",
        "\n",
        "train_features = convert_examples_to_features(train_texts, train_labels)\n",
        "train_dataset = convert_features_to_tf_dataset(train_features)\n",
        "\n",
        "validation_features = convert_examples_to_features(validation_texts, validation_labels)\n",
        "validation_dataset = convert_features_to_tf_dataset(validation_features)\n",
        "\n",
        "train_dataset = train_dataset.shuffle(100).batch(32)\n",
        "validation_dataset = validation_dataset.batch(32)\n",
        "\n",
        "# Visualizamos un bacth de 32 ejemplos\n",
        "#instance = list(train_dataset.take(1).as_numpy_iterator())\n",
        "#print(instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb9BcKNyw8CS"
      },
      "source": [
        "⬛\n",
        "####EJERCICIO 2\n",
        "¿Qué es lo que hace y retorna como salida la función `tokenizer.batch_encode_plus`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oystNrMyxIfW"
      },
      "source": [
        "[RESPONDER AQUÍ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GorwYEWOcFWs"
      },
      "source": [
        "## 4. Entendiendo el tokenizer\n",
        "\n",
        "Cuando preprocesamos el texto de entrada para ser introducido en un encoder como BERT, típicamente seguimos tres pasos:\n",
        "\n",
        "1.   Dividir las palabras en tokens (subwords).\n",
        "2.   Agregar los tokens especiales como [CLS] y [SEP]. Estos tokens especiales ya están incluidos en el vocabulario del modelo, por lo que tienen su propio identificador de token.\n",
        "3.   Sustituir los tokens por sus identificadores correspondientes. Después de este paso, obtenemos la forma adecuada para BERT.\n",
        "\n",
        "El siguiente código muestra los resultados de estos tres pasos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFDfUY1UcWzS"
      },
      "outputs": [],
      "source": [
        "sentence1 = \"A visually stunning rumination on love.\"\n",
        "sentence2 = \"There ought to be a directing license, so that Ed Burns can have his revoked.\"\n",
        "\n",
        "print('0. INPUT SENTENCE: {}'.format(sentence1))\n",
        "\n",
        "# Tokenizamos una frase\n",
        "sentence1_tokenized = tokenizer.tokenize(sentence1)\n",
        "print('1. TOKENIZED SENTENCE: {}'.format(sentence1_tokenized))\n",
        "\n",
        "# Añadimos tokens especiales\n",
        "sentence1_tokenized_with_special_tokens = ['[CLS]'] + sentence1_tokenized + ['[SEP]']\n",
        "print('2. ADD [CLS], [SEP]: {}'.format(sentence1_tokenized_with_special_tokens))\n",
        "\n",
        "# Convertimoa los tokens a ids\n",
        "sentence1_ids = tokenizer.convert_tokens_to_ids(sentence1_tokenized_with_special_tokens)\n",
        "print('3. SENTENCE IDS: {}'.format(sentence1_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtrYoRN_o_jd"
      },
      "source": [
        "⬛\n",
        "####EJERCICIO 3\n",
        "¿Qué le ocurrió a \"rumination\" después de la tokenización? ¿Por qué?\n",
        "¿Cuáles son los identificadores de los tokens [CLS] y [SEP]?\n",
        "¿De dónde vienen los identificadores de token?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqVaYzbgpVhY"
      },
      "source": [
        "[RESPONDER AQUÍ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmPo2_4twYmE"
      },
      "source": [
        "Los tres pasos se pueden realizar con las funciones `encode` o `batch_encode_plus`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTYbni402ES0"
      },
      "outputs": [],
      "source": [
        "sentence1_ids = tokenizer.encode(sentence1, add_special_tokens=True)\n",
        "print('SENTENCE IDS: {}'.format(sentence1_ids))\n",
        "\n",
        "batch_encoding = tokenizer.batch_encode_plus(\n",
        "        [sentence1], max_length=128, pad_to_max_length=True,\n",
        "    )\n",
        "print('ENCODE PLUS: {}'.format(batch_encoding))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2TuL2KEqeYq"
      },
      "source": [
        "⬛\n",
        "####EJERCICIO 4\n",
        "¿Qué diferencias hay entre las funciones `tokenizer.encode` y `tokenizer.batch_encode_plus`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuijtGn0rHAq"
      },
      "source": [
        "[RESPONDER AQUÍ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhRQkPRa2KBL"
      },
      "source": [
        "### Dos frases como entrada\n",
        "\n",
        "Como hemos visto en la clase de teoría, BERT es un modelo de lenguaje enmascarado que aprende prediciendo palabras ocultas y, además, determina si una segunda frase sigue a una primera. Por esta razón, el tokenizer de BERT está diseñado para aceptar dos oraciones como entrada. Este tipo de preprocesamiento de datos es especialmente útil para tareas como similitud textual semántica (*semantic textual similarit*y), inferencia en lenguaje natural (*natural language inference*, *textual entailment*), o búsqueda de respuesta (*question answering*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2qZJTkFq8y6"
      },
      "outputs": [],
      "source": [
        "sentence_pair_ids = tokenizer.encode(text=sentence1, text_pair=sentence2, add_special_tokens=True)\n",
        "batch_encoding = tokenizer.batch_encode_plus(\n",
        "        [(sentence1, sentence2)], max_length=128, pad_to_max_length=True,\n",
        "    )\n",
        "\n",
        "print(\"SENTENCE PAIR IDS: {}\".format(sentence_pair_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSeX9VzaFOAY"
      },
      "source": [
        "⬛\n",
        "####EJERCICIO 5\n",
        "¿Qué IDs corresponden a la primera frase y cuáles a la segunda? ¿Cómo quedan separados?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9IUBGQcLVir"
      },
      "source": [
        "[RESPONDER AQUÍ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D80JtwZbWJ7C"
      },
      "source": [
        "## 5. Ajuste fino (fine-tuning) de BERT como un clasificador de frases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvcNYaASM0oL"
      },
      "source": [
        "Una característica interesante al ajustar un modelo grande es que no necesitamos entrenar durante muchas épocas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vWeYpxdCmtC"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "history = model.fit(train_dataset, epochs=3, validation_data=validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMfta2vyKCXt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visiualizamos el loss sobre los conjuntos de entrenamiento y validación en las 3 épocas de entranmiento\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Visiualizamos el accuracy sobre los conjuntos de entrenamiento y validación en las 3 épocas de entranmiento\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EsEkO3uG1et"
      },
      "source": [
        "⬛\n",
        "####EJERCICIO 6\n",
        "Una vez que el modelo esté ajustado para el análisis de sentimientos, podemos evaluarlo en el conjunto de test. Para ello, también necesitamos tokenizar la entrada y convertirla a IDs. Escribe a continuación el código correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KFUtfXzImAY"
      },
      "outputs": [],
      "source": [
        "# Recordatorio del modelo y tokenizer cargados arriba\n",
        "#model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# AÑADIR CÓDIGO AQUÍ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jcr5u1b5EuE"
      },
      "source": [
        "⬛\n",
        "####EJERCICIO 7\n",
        "Considerar el modelo `TFBertModel` en vez de `TFBertForSequenceClassification`.\n",
        "\n",
        "Modifica la arquitectura de `TFBertModel` para replicar los resultados obtenidos por `TFBertForSequenceClassification` en el ejercicio 5.\n",
        "\n",
        "Para ello, tienes que añadir una capa feed-forward para clasificación binaria, que reciba como entrada la representación del token [CLS] generada por `TFBertModel`. Además, debes especificar las entadas `input_ids`, `attention_mask` y `token_type_ids`, así como decidir si añades `dropout` al clasificador, y si usas alternativas a `optimizer` y `loss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NruPrZ2r5Mn8"
      },
      "outputs": [],
      "source": [
        "from transformers import TFBertModel\n",
        "\n",
        "# AÑADIR CÓDIGO AQUÍ\n",
        "\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# AÑADIR CÓDIGO AQUÍ\n",
        "\n",
        "history = model.fit(train_dataset, epochs=3, validation_data=validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umpIzzvR_F-w"
      },
      "outputs": [],
      "source": [
        "# Visiualizamos el loss sobre los conjuntos de entrenamiento y validación en las 3 épocas de entranmiento\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Visiualizamos el accuracy sobre los conjuntos de entrenamiento y validación en las 3 épocas de entranmiento\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
